{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aab7411",
   "metadata": {},
   "source": [
    "#   <p style=\"text-align: center;\">  FEDERATED LEARNING WITH PYTORCH </p>\n",
    "\n",
    "In this notebook, we will implement a simple federated learning (FL) simulation, as described in [1].\n",
    "\n",
    "Let us first start with defining the required parameters of a federated learning system and explain how they are utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b26c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FL system parameters\n",
    "num_users = 10\n",
    "local_training_iters = 5\n",
    "global_epochs = 15\n",
    "fraction = 1\n",
    "\n",
    "\n",
    "#model parameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9 \n",
    "batch_size = 32\n",
    "test_batch_size =64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e0be3f6",
   "metadata": {},
   "source": [
    "Now that we have the parameters, let us go over how an FL system operates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9c3b9",
   "metadata": {},
   "source": [
    "![FL](fl.png)\n",
    "Fig.1 An FL system at any given iteration. Time index is omitted for convenience. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee71ab5",
   "metadata": {},
   "source": [
    "In a traditional FL system, there is one central server and ```num_users``` ($N$) many users, each of whom has a local dataset (which are depicted by images in Fig. 1). The protocol runs for ```global_epochs``` many rounds. During each round $t$, each user $i\\in [N]$ performs model training for ```local_training_iters``` rounds and computes the local model update $w_i^{t}$ via mini-batch gradient descent. Then, they send the freshly computed model parameters to the server. The server, in turn averages these model updates [<sup>1</sup>](#fn1) and computes the new global model: \n",
    "$$w^{t+1}=\\frac{1}{N}\\sum_{i\\in[N]}w_i^{t}$$\n",
    "Then, the server sends the new global model back to the users for the next round. This concludes one global round of FL. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e58373",
   "metadata": {},
   "source": [
    "In the original protocol, the server randomly chooses only a subset (of size ```fraction``` $\\times$ ```num_users```) of the users to perform local training and send in their model updates at each round. This ensures that communication and computation resources of the users are being leveraged effectively. With this in mind, the actual update rule for iteration $t$ is as follows: $$w^{t+1}=\\frac{1}{N}\\sum_{i\\in\\mathcal{S}^{t}}w_i^{t},$$ where $\\mathcal{S}^{t}\\subseteq[N]$ denotes the set of selected users at iteration $t$. Note that for ```fraction=1```, there is no client selection and the two updates rules are equivalent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361f5b6",
   "metadata": {},
   "source": [
    "There are many benefits of using FL for distributed learning:\n",
    "1. Communication-efficiency: Since training occurs on the user-side, users do not share their datasets with any other party and they only share their model update with the server at the end of every ```local_training_iters``` iterations, instead of at the end of each iteration. Moreover, with the client selection property of FL, only a subset of the users participate at each global round. The combination of these three properties make FL a communication-efficient distributed learning paradigm. \n",
    "2. Computation-efficiency: The client selection property of FL also allows for a computation efficient distributed learning framework since even if one user does not participate the training process at a given round, the model is still being trained by the other users. Thus, a non-participating user still receives a global model without running no local training iteration.\n",
    "3. Privacy: On-device learning architecture of FL allow data to remain on the user-side, which achieves a degree of privacy[<sup>2</sup>](#fn2) since the data is not observed by any other party. \n",
    "4. Customization: Federated learning protocols can easily be customized to meet specific needs because the protocol is very granular. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8d9ad",
   "metadata": {},
   "source": [
    "Now that we have discussed the overall properties of an FL framework, let us get our hands dirty with some coding. For the sake of this notebook, we will not implement client selection and leave it to next notebook, where we will explore the most popular client selection strategies that are used with FL and implement them.\n",
    "\n",
    "\n",
    "For this basic FL framework, we will implement 4 parts. \n",
    "1. Data handling\n",
    "2. Model architecture\n",
    "3. Local training loop\n",
    "4. Model averaging\n",
    "\n",
    "\n",
    "We first start with importing the required libraries. The code below used [2] as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce8fedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import copy\n",
    "from torch.utils.data.sampler import SubsetRandomSampler \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3479a288",
   "metadata": {},
   "source": [
    "### DATA HANDLING\n",
    "For this example, we will use the well-known Fashion MNIST [3] dataset. Fortunately, PyTorch provides built-in datasets and Fashion MNIST is one of them. To read more about which datasets are offered and how they can be used, read the <a href=\"https://pytorch.org/vision/stable/datasets.html\">docs</a> and <a href=\"https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\">the tutorial</a>. With the piece of code below, we are able to download the dataset, normalize it and split it into train and test datasets with ease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9cca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset_train = datasets.FashionMNIST(\"../data/fashion/\", train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.FashionMNIST(\"../data/fashion/\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06106e19",
   "metadata": {},
   "source": [
    "The next step is to split the training data into $N$ chunks and assign each user a chunk. For the purposes of this notebook, we will randomly divide the dataset into equally-sized chunks, which means that the dataset distribution of each user will be similar to of each other's, which we also term as independent identically distributed or IID for short.[<sup>3</sup>](#fn3)\n",
    "\n",
    "We now randomly selected indices of the Fashion MNIST dataset that will belong to each user. Specifically, the indices of the Fashion MNIST training data that will be owned by user $i$ is stored in ```user_train_data_indices[i]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a4fd624",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_items_in_each_chunk = int(len(dataset_train)/num_users)\n",
    "user_train_data_indices = {}\n",
    "indices = np.arange(len(dataset_train))\n",
    "for i in range(num_users):\n",
    "    user_train_data_indices[i] = set(np.random.choice(indices, number_of_items_in_each_chunk,  replace = False))\n",
    "    indices = list(set(indices) - user_train_data_indices[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e81e8",
   "metadata": {},
   "source": [
    "Finally, we will set up the data loaders we will use to iterate over the data. A data loader is a useful tool, which is provided by PyTorch so we can easily split the data into batches for mini-batch training and iterate over the data points. I highly recommend reading the <a href=\"https://pytorch.org/docs/stable/data.html\"> documentation</a>. \n",
    "\n",
    "We use the previously computed indices, which are stored in ```user_train_data_indices```, to create one ```SubsetRandomSampler``` per user to distribute the training dataset to users. Note that ```SubsetRandomSampler``` is a class that allows sampling from a dataset given some indices. We can then give the corresponding sampler as input to the train data loader of one user. Therefore, we create one training data loader and one sampler for each user since each user has their own dataset, and one test data loader for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7274fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset_test, batch_size=test_batch_size)\n",
    "train_dataloaders = []\n",
    "\n",
    "for i in range(num_users):\n",
    "    smplr = SubsetRandomSampler(list(user_train_data_indices[i]))\n",
    "    train_dataloaders.append(DataLoader(dataset_train, batch_size=batch_size, shuffle=False, sampler = smplr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf8d64",
   "metadata": {},
   "source": [
    "At this point, each user has a random non-overalapping subset of the Fashion MNIST training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e12456",
   "metadata": {},
   "source": [
    "### MODEL ARCHITECTURE\n",
    "\n",
    "We will use a simple CNN architecture with two convolutional layers with 5x5 kernels and 10, 32 channels respectively. Then, we implement a fully connected layer with 512 units with the ReLU activation and finally an output layer with softmax activation. The network architecture is inspired from the CNN architecture that is used for training on MNIST in [1]. We use PyTorch for implementing and training the network. The <a href=\"https://pytorch.org/docs/stable/nn.html\">docs</a> and <a href=\"https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\">the tutorial</a> on this subject are also very well-written. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c07a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 32, kernel_size=5)\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(512, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50b369",
   "metadata": {},
   "source": [
    "### LOCAL TRAINING LOOP\n",
    "\n",
    "With the help of the data loaders that we defined, we define the training function that performs mini-batch training using its inputs: the dataset, the model parameters, the loss function for optimization and the optimizer of choice. \n",
    "\n",
    "In the next section of our code, each user will call this function ```local_training_iters``` times at each global round that they were selected to perform training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7cc70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss/num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120075d",
   "metadata": {},
   "source": [
    "### MODEL AVERAGING\n",
    "\n",
    "We made it to the actual model averaging stage of federated learning! In this step, we will tie everything that we have done together and finish our FL implementation! Let us first configure the device that we will run the training on. As default, I set ```gpu_id=-1```, which means that the training will run on CPU. If you have a GPU, feel free to change ```gpu_id``` to the ID of the GPU that you will use!\n",
    "\n",
    "\n",
    "We then initialize the model randomly, define some variables that we will use later and define the loss function as the cross entropy loss as we will perform a classification task. \n",
    "\n",
    "The model operations defined on this part are inspired from [this](https://github.com/pytorch/pytorch/blob/master/torch/optim/swa_utils.py) code, which I highly suggest you to go over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cc0876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = -1\n",
    "device =  torch.device(\"cuda:{}\".format(gpu_id)\n",
    "            if torch.cuda.is_available() and gpu_id != -1\n",
    "            else \"cpu\"\n",
    "        )\n",
    "local_model = CNN().to(device)\n",
    "global_model = copy.deepcopy(local_model)\n",
    "avg_model = copy.deepcopy(local_model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_train = []\n",
    "\n",
    "## Below are some efficient and safe operations on tensors for operations on models ##\n",
    "\n",
    "def copy_model(local_model, global_model):\n",
    "    global_param = global_model.parameters()\n",
    "    local_param = local_model.parameters()\n",
    "    for local_t, global_t in zip(local_param, global_param):\n",
    "        local_t.detach().copy_(global_t)\n",
    "\n",
    "def zero_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.detach().zero_()\n",
    "\n",
    "def add_model(model_a, model_b):\n",
    "    for a_t, b_t in zip(model_a.parameters(), model_b.parameters()):\n",
    "        a_t.detach().add_(b_t)\n",
    "\n",
    "def print_model(model):\n",
    "    for param in model.parameters():\n",
    "        print(f\"params: {param}\")\n",
    "\n",
    "def div_model(model, divisor):\n",
    "    for param in model.parameters():\n",
    "        param.detach().mul_(1.0/divisor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea7a63",
   "metadata": {},
   "source": [
    "Here, each user calls the ```train_loop``` function with their local datasets for ```local_training_iters``` times to perform local training with their own optimizer instance. In this case, each instance is an SGD with the same learning rate.[<sup>4</sup>](#fn4)  \n",
    "Then, the models are averaged at the end of all local training iterations. At the end of one global round, the global model is sent to all of the users for the next round of training.\n",
    "The process described above is carried out for ```global_epochs``` many epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c2973f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b4768e302a470e8d0dff70d9054ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dabf790ede4e7994a844433e7c99f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1-Average train loss: 0.779983\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe82df0cf62473987aa5b0eb437b3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2-Average train loss: 0.473625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0a9ef79851434ca744b57f2baacdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3-Average train loss: 0.392629\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c73043adf94de684a544511ec9086a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4-Average train loss: 0.349972\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4770b9e1aa40fd918f5b0e3016b31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5-Average train loss: 0.325625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55efb4b7bc0348f198d199dc24972273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6-Average train loss: 0.305613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a81c895b0747768e777c89231f57ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7-Average train loss: 0.290924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43bbc74bc0444c8a33352428d233599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8-Average train loss: 0.278766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc97814c3de1425e8f72b5312f2b8cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9-Average train loss: 0.268172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b9ea2d950043e0aac325da7cd0e163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10-Average train loss: 0.258689\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5499f6bbaea04874a69998004f9c47df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11-Average train loss: 0.252025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5c980f0c364e819071eec5795b0169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12-Average train loss: 0.243581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c337a32836984a7b969e3971957ec1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13-Average train loss: 0.237437\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe84965ae51e45bba50ef13c28e10552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14-Average train loss: 0.233255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c763113157441ab196a2154799e8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15-Average train loss: 0.227812\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(global_epochs)):\n",
    "    loss_avg = 0\n",
    "    zero_model(avg_model)\n",
    "    for id in tqdm_notebook(range(num_users)):\n",
    "        copy_model(local_model, global_model)\n",
    "        optimizer = torch.optim.SGD(local_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        for _ in range(local_training_iters):\n",
    "            loss_avg += train_loop(train_dataloaders[id], local_model, loss_fn, optimizer)\n",
    "        add_model(avg_model, local_model)\n",
    "\n",
    "    div_model(avg_model, num_users)\n",
    "    copy_model(global_model, avg_model)\n",
    "    loss_avg /= num_users*local_training_iters\n",
    "    loss_train.append(loss_avg)\n",
    "    print(f\"Epoch {i+1}-Average train loss: {loss_avg:>7f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa9d5fc",
   "metadata": {},
   "source": [
    "Now that the training process is done, let us plot the training loss and compute the test accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14d3d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(loss_train)), loss_train)\n",
    "plt.ylabel('train_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694b6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_img(net_g, datatest):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=test_batch_size)\n",
    "    l = len(data_loader)\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        if gpu_id != -1:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    \n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5bb346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(90.3500)\n"
     ]
    }
   ],
   "source": [
    "acc, loss = test_img(global_model, dataset_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38df17e",
   "metadata": {},
   "source": [
    "Not bad at all! This concludes our little experiment with federated learning! Let me know what you thought of this little snippet, your feedback is soooo valuable to me. Next time, we will turn this code into a Python script and experiment with different values and visualize our results. Stay tuned! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8174d53",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "[<sup id=\"fn1\">1</sup>](#fn1) Depending on the system configurations, a weighted averaging scheme might be preferred. For example, in many applications of federated learning, the weight of the model update of a user is directly proportional with the local dataset size of the user.\n",
    "<!--  </span> -->\n",
    " \n",
    " [<sup id=\"fn2\">2</sup>](#fn2) <span id=\"fn2\"> There are still some privacy issues associated with FL because model updates carry some information about the datasets of the users. However, in comparison to other distributed learning paradigms where data is shared with other parties, FL allows a certain degree of privacy. </span>\n",
    " \n",
    "[<sup id=\"fn3\">3</sup>](#fn3) <span id=\"fn3\"> The distribution of the dataset to users actually impacts the performance of federeated learning quite a bit. We will learn about this more in the upcoming lessons. But for now, for simplicity, we will only work with IID data for the users. Dealing with non-IID data distributions is an open area of research for FL. There are some resources [4, 5, 6] in the references in case you would like to check them out.</span>\n",
    "\n",
    "[<sup id=\"fn4\">4</sup>](#fn4) <span id=\"fn4\"> There are many use cases where the learning rates of different users might be different, such as [7].</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26b214",
   "metadata": {},
   "source": [
    "### References "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e8f26",
   "metadata": {},
   "source": [
    "[1] McMahan, Brendan, et al. \"Communication-efficient learning of deep networks from decentralized data.\" Artificial intelligence and statistics. PMLR, 2017.\n",
    "\n",
    "[2] Shaoxiong Ji. (2018, March 30). A PyTorch Implementation of Federated Learning. Zenodo.\n",
    "\n",
    "[3] Xiao, Han, Kashif Rasul, and Roland Vollgraf. \"Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.\" arXiv preprint arXiv:1708.07747 (2017).\n",
    "\n",
    "[4] Li, Xiang, et al. \"On the convergence of fedavg on non-iid data.\" arXiv preprint arXiv:1907.02189 (2019).\n",
    "\n",
    "[5] Karimireddy, Sai Praneeth, et al. \"Scaffold: Stochastic controlled averaging for federated learning.\" International Conference on Machine Learning. PMLR, 2020.\n",
    "\n",
    "[6] Li, Xiaoxiao, et al. \"Fedbn: Federated learning on non-iid features via local batch normalization.\" arXiv preprint arXiv:2102.07623 (2021).\n",
    "\n",
    "[7] Xu, Chunmei, et al. \"Learning rate optimization for federated learning exploiting over-the-air computation.\" IEEE Journal on Selected Areas in Communications 39.12 (2021): 3742-3756."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d645db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
